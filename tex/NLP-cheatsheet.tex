\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{graphicx}

% To make this come out properly in landscape mode, do one of the following
% 1.
%  pdflatex latexsheet.tex
%
% 2.
%  latex latexsheet.tex
%  dvips -P pdf  -t landscape latexsheet.dvi
%  ps2pdf latexsheet.ps


% If you're reading this, be prepared for confusion.  Making this was
% a learning experience for me, and it shows.  Much of the placement
% was hacked in; if you make it better, let me know...


% 2008-04
% Changed page margin code to use the geometry package. Also added code for
% conditional page margins, depending on paper size. Thanks to Uwe Ziegenhagen
% for the suggestions.

% 2006-08
% Made changes based on suggestions from Gene Cooperman. <gene at ccs.neu.edu>


% To Do:
% \listoffigures \listoftables
% \setcounter{secnumdepth}{0}


% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
	{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}

% Turn off header and footer
\pagestyle{empty}
 

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}


% -----------------------------------------------------------------------

\begin{document}

\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\textbf{NLP Cheat Sheet}} \\
\end{center}

\section{Bag of Words Methods}
\subsection{Information Retrieval}
\subsubsection{Cosine similarity metric}
Define a similarity metric between topic vectors. It is the cosine of the 
angle between the term vectors.

\begin{equation*}
sim(A, B) = \frac{\sum_{i} a_i \times b_i}{\sqrt{\sum_{i} a_i^2} \times \sqrt{\sum_{i} b_i^2}}
\end{equation*}

Unusual words determine the topic much more than common words.
Weight each term frequency $tf_i$ by its inverse document frequency $idf$.
\begin{equation*}
idf_i = \log (\frac{N}{n_i})
\end{equation*}

where $N$=size of collection and $n_i$ = number of documents containing term  $i$
\begin{equation*}
w_i = tf_i \times idf_i
\end{equation*}

\resizebox{\textwidth/3}{!}{ 
$sim(A, B) = \frac{\sum_{w \in A,B} tf_{w,A} \times tf_{w,B} \times idf_w^2}{\sqrt{\sum_{w_1 \in A} (tf_{w_1,A} idf_{w_1})^2} \times \sqrt{\sum_{w_2 \in B} (tf_{w_2,A} idf_{w_2})^2}}$
}

\subsection{Sentiment analysis (opinion mining)}

Opinion mining is the task of judging whether a document expresses a positive or a negative opinion (or no opinion) regarding a particular object or topic. 

The simplest strategy uses a bag-of-words model. We create lists of 'positive' and 'negative' words and judge a document based on whether it has a preponderance of positive or negative words (and judge it neutral or 'objective' if it has few words of either category.

Creating such lists is not easy; some of the words are likely to be quite different for different kinds of topics. As an alternative, we will take a collection of documents on some topic and rate them (by hand) as positive or negative. We will then use this collection to train a classifier model.

$
c = argmax_{c \in C} P(c) \prod_{i \in positions} P(w_i | c)	 
$ \\

$P(c) = \frac{count(\text{docs labeled t})}{N}$ \\

$P(w_i | c) = \frac{count(\text{docs labeled t containing } w_i)}{count(\text{docs labeled t})}$

\textbf{Smoothing}: $P(w_i | c) = \frac{count(w_i, c) + 1}{(\sum_{w \in V} count(w,c)) + |V|}$ \\

The bag-of-words strategy with NB model does quite well for short reviews, but fails for:
\begin{itemize}
	\item ambiguous terms
	\item negation
	\item comparative reviews
	\item revealing aspects of an opinion
\end{itemize}


\section{Context-free grammar}
A context-free grammar consists of:
\begin{itemize}
	\item a set of non-terminal symbols $A, B, C, \dots \in N$
	\item a set of terminal symbols $a, b, c, \dots \in T$
	\item a start symbol $S \in N$
	\item – a set of productions $P$ of the form $N \rightarrow (N U T)*$
\end{itemize}
CFG can be used to generate sentences, recognize sentences or parse sentences.

\section{Parsing}
\subsection{The CKY algorithm}
The	grammar	has	to	be	in	\textbf{Chomsky	normal form (CNF)}. Every production must have the form $A \rightarrow B C$ or $A \rightarrow a$.

\begin{itemize}
	\item Use a parse table $PT[i,j]$
	\begin{itemize}
		\item each entry is a list of grammar symbols
		\item $PT[i,j]$ includes $x$ if the words from $i$ up to (but not including) $j$ can be analyzed as an $x$
		\end{itemize}		

	\item Two steps
		\begin{itemize}
		\item if there is a rule $A \rightarrow a$ and word $i$ is an $a$, put an $A$ in $PT[i, i+1]$
		\item if there is a rule $A \rightarrow B C$, $PT[i,j]$ includes $A$ and $PT[j,k]$ includes $B$, add $C$ to $PT[i,k]$
		\end{itemize}		
\end{itemize}



Time complexity = $O(n^3)$ \\
Space complexity = $O(n^2)$

\section{POS tagging}

\subsection{Accuracy}
 for part-of-speech tagging, accuracy is a simple and reasonable metric.
 
 $accuracy = \frac{\text{tokens with correct tag}}{total tokens}$
\subsection{Precision and recall}

Instead of counting the tags themselves, we count the names defined by these tags: 
key = number of names in key 
response = number of names in system response
correct = number of names in response which exactly match (in type and extent) a name in the key

$precision = correct / response$ \\
$recall = correct / key$ \\
Also
$precision = \text{True Positives} / (\text{True Positives} + \text{False Positives})$ \\
$recall = \text{True Positives} / (\text{True Positives} + \text{False Negatives})$

\textbf{F-measure}: the harmonic mean of 
recall and precision.
$F = 2 \times \frac{precision \times recall}{precision + recall}$

\section{HMMs and the Viterbi decoder}
HMM is a Weighted Finite-state Automaton (WFSA)
\begin{itemize}
	\item Each transition arc is associated with a probability
	\item The sum of all arcs outgoing from a single node is 1
\end{itemize}

Viterbi decoder's time complexity is $O(s^2 n)$ and the space complexity is $O(s n)$ for $s$ states and $n$ words.

$viterbi[s, t] = max_{s'}( viterbi[s', t-1] \times p(s, s') \times P (token[t] | s) )$

for each $s, t$: record which $s’, t-1$ contributed the maximum.

\section{Chunkers and name taggers}

\subsection{BIO (IOB) tags}

$I \rightarrow \text{inside a baseNP}$ \\ 
$O \rightarrow \text{outside a baseNP}$ \\
$B \rightarrow \text{start of a baseNP which follows another baseNP}$

\section{Maximum entropy}
MaxEnt is typically used for a multi-class classifier.  We are given a set of training data, where each datum is labeled with a set of features and a class (tag).  Each feature-class pair constitutes an indicator function.  We train a classifier using this data, computing the αs.  We can then classify new data by selecting the class (tag) which maximizes $p(h ,t)$.

MaxEnt models can be trained by providing a set of $K$ features in the form of binary-valued indicator functions $f_i (h, t)$.

We will use a log-linear model, where $p(h, t) = (1/Z) \prod_{i=1}^K \alpha_i^{f_i(h, t)}$

where $\alpha_i$ is the weight for feature $i$, and $Z$ is a normalizing constant.

\subsection{Maximum Entropy Markov Models (MEMM)}

Maximum entropy modeling can be combined with a Markov model, so that, for each state, the probability of a transition to that state is computed by a Max Ent model, based on the prior state and arbitrary features of the input sequence.  The result is a Maximum Entropy Markov Model (MEMM).  Decoding (selecting the best tag sequence) can be done deterministically left-to-right or (better) using Viterbi decoding like an HMM.

\subsubsection{HMM vs MEMM}
HMM is a generative model, while MEMM is a discriminative model.
MEMM can incorporate long distance features.

\subsection{Feature engineering}

When using a package such as MaxEnt, the computational linguist's job becomes one of feature engineering -- identifying the features which will be most predictive of the tags we are trying to assign. Simple features are the current token (or the previous token, or the next token) having a particular value, or being on a particular list (such as a list of common titles or common first names).


\section{Lexical semantics and word sense disambiguation}

\subsection{Terminology}
\begin{itemize}
	\item multiple senses of a word
	\item polysemy (and homonymy for totally unrelated senses ("bank"))
	\item metonomy for certain types of regular, productive polysemy ("the White House", "Washington")
	\item zeugma (conjunction combining distinct senses) as test for polysemy ("serve")
	\item synonymy:  when two words mean (more-or-less) the same thing
	\item hyponymy:  X is the hyponym of Y if X denotes a more specific subclass  of Y (X is the hyponym, Y is the hypernym)
	\item synsets: synonym sets
\end{itemize}

\subsection{Word Sense Disambiguation}
process of identifying the sense of a word in context

Simple supervised WSD algorithm:  naive Bayes:

$\text{selected sense s'} = argmax(sense s) P(s | F) = argmax(s) P(s) \prod_i P(f[i]|s)$ where $F = \{f1, f2, \cdots\}$ is the set of context features

Training a naive Bayes classifier consists of estimating each of these probabilities.

$P(s_i) = \frac{count(s_i, w_j)}{count(w_j)}$ \\
Number of times the sense $s_i$ occurs in $w_j$ and divide by the total count of the target word $w_j$.

$P(f_j|s) = \frac{count(f_j, s)}{count(s)}$

\subsection{Word Embeddings}
Word embeddings are low-dimensional (50-200 element) real vectors that encode information on the contexts in which a word appears. Each component of the vector combines information on multiple contexts.

The embeddings can be produced by dimensionality reduction on the context matrix.
\subsection{Information Content}

$P(c)$: for each concept (synset), $P(c)$ = probability that a word in a corpus is an instance of the concept (matches the synset $c$ or one of its hyponyms)

Information content of a concept
$IC(c) = -\log P(c)$


\section{Probabilistic CFG}

Associate a probability with each production 
\begin{itemize}
	\item  sum of probabilities of productions expanding a symbol = 1
	\item use MLE
\end{itemize}

$P(A \rightarrow \alpha) = count(A \rightarrow \alpha) /  \sum_{\beta} count(A | \beta )$ 

Probability of parse = product of prob. of productions

\section{Learning}

\textbf{Supervised learning}: All training data is labeled \\

\textbf{Semi-Supervised learning} \begin{itemize}
	\item Part of training data is labeled
	\item Make use of redundancies to learn labels of additional data, then train model
	\item Co-training
	\item Reduces amount of data which must be hand-labeled to achieve a given level of performance
\end{itemize} 

\textbf{Active learning}: 
\begin{itemize}
	\item Start with partially labeled data
	\item System selects additional 'informative' examples for user to label
\end{itemize}

\textbf{Unsupervised learning}: driven entirely by an unlabeled corpus

\textbf{Distant supervision}: given a large amount of tabular information, we use a heuristic procedure to (partially) label a text corpus.
Distant supervision works roughly like a half iteration of bootstrapping, but with a large set of seeds. Given a data base with a large set of instances of a relation and a large raw text corpus, we use the instances to label the corpus and then train a classifier over the corpus.


\textbf{Confidence of pattern P}:
$Conf(P) = \frac{P.positive}{P.positive + P.negative}$
where P.positive = number of positive matches to pattern P and P.negative = number of negative matches to pattern P


\section{Reference resolution}
Identify all phrases which refer to the same real-word entity.

\subsection{Terminology}

Referent: real world object referred to 

Referring expression [mention]: a phrase referring to that object

Discourse entities:  the set of objects referred to by a text

Coreference: two expressions referring to the same thing.

Antecedent: prior expression

Anaphor: following expression

\textbf{Types of referring expressions}: definite pronouns (he, she, it), indefinite pronouns (one), definite NPs (the car), indefinite NPs (a car), names.

\textbf{Wikification}: entity linking, so map each document-level entity to an entry in a standard database as Wikipedia

\subsection{Complications}
\begin{itemize}
	\item inferrables:  sometimes the relation between anaphor and antecedent is not one of identity
	\item  zero Anaphora: many languages allow subject omission, and some allow omission of other arguments
	\item Cataphora: pronoun referring to a following mention
	\item Bridging Anaphora: reference to related object
	\item Non-NP Anaphora: pronouns can also refer to events or propositions
\end{itemize}

\subsection{Hobbs search}
In selecting which sentences to search, Hobbs starts with the sentence containing the anaphor and then selects prior sentences from right to left, examining first the immediately preceding sentence.
However, within an individual sentence he searches from left to right;  this is the way he captures the preference for antecedents in subject position.
The resulting search order defines a Hobbs distance:  if in searching for antecedents of $X$ the $d-th$ candidate we consider is $Y$, we say that the Hobbs distance from $X$ to $Y$ is $d$.

\subsection{Resolving pronoun reference}
Constraints:
\begin{itemize}
	\item animacy
	\item gender
	\item number
\end{itemize}

Preferences:
\begin{itemize}
	\item recent: at most 3 sentences back
	\item salient: mentioned several times recently
	\item subjects: 
\end{itemize}

Selectional preferences: prefer antecedent that is more likely to occur in context of pronoun.

\textbf{Ge, Hale, and Charniak formula}:
$P = P1( \text{correct antecedent is at Hobbs distance d}) \times 
P2( \text{pronoun} | \text{head of antecedent}) \times
P3( \text{antecedent} | \text{mention count}) \times
P4( \text{head of antecedent} | \text{context of pronoun} ) $

\subsection{Models}
\begin{itemize}
	\item mention-pair model: train binary classifier
		\begin{itemize}
			\item scan mention in text order
			\item link each mention to the closest antecedent classified
			\item link each mention to antecedent most confidently labeled
			\item cluster mentions
		\end{itemize}
	\item entity-mention model: uses information from all mentions gathered so far
\end{itemize}


\section{Question Answering}
\subsection{Mean Reciprocal Rank}
Typically for evaluations QA systems generate a ranked set of answers to each query and are scored based on the rank of the first correct answer:  mean reciprocal rank

\begin{equation*}
	MRR = (\frac{1}{N}) \sum_i \frac{1}{rank_i}
\end{equation*}
Each question is then scored according to the reciprocal of the rank of the first correct answer. The score of a system is then the average of the score for each question in the set.


\section{Machine translation}
\subsection{The Noisy Channel Model}
Suppose we are translating French/Foreign sentences \textbf{F} to English \textbf{E}
We want to determine the English sentence most likely to have generated it:
\begin{align*}
E' &= argmax_E P(E | F) \\
   &= argmax_E \frac{P(F | E) P(E)}{P(F)} \\
   &= argmax_E P(F | E) P(E)
\end{align*}

It combines \textbf{translation model} $P(F | E)$ and \textbf{language model} $P(E)$.
$P(F|E)$ is learned from parallel corpus, $P(E)$ can be learned from large monolingual corpus.

\subsection{The Lexical Translation Model}
\subsubsection{IBM Model 1}
Translation is based on alignment. Mapping a target word at position $i$ to a source word at position $j$.
Assumptions:
\begin{itemize}
	\item Each target word is generated by exactly one source word 
	\item  A target word can be generated by a NULL word; multiple target words can be generated by the same source word
	\item All alignments are equally likely (a very crude model)
\end{itemize} 

Translation model generates a sentence of F (given E) in 3 steps:
\begin{itemize}
	\item pick a length for F
	\item pick an alignment of F (length J) and E (length I+1) $P(A|E) = \frac{\epsilon}{(I+1)^J}$
	\item pick the $ith$ word of F based on English word $e_j$ with which it aligns using distribution $t(f_i | e_j)$
		$P(F| E,A) = \prod_j t(f_j, e_{a_j})$\\
\end{itemize}

\subsubsection{EM training}
Typically we are given sentence aligned but not word aligned corpora, so we cannot compute these probabilities by direct counting and instead use an iterative procedure called "EM".

\begin{equation} \label{eq:1}
P(F, A | E) = P(F| E,A) \times P(A|E)
			= \frac{\epsilon}{(I+1)^J} \prod_j t(f_j, e_{a_j})
\end{equation}

\begin{equation} \label{eq:2}
P(A| F, E) =\frac{P(F, A | E)}{\sum_A P(F, A|E)}
\end{equation}

\begin{equation*}
P(F|A) = \sum_A P(F, A | E) / P(A|F,E) = \frac{P(F, A | E)}{\sum_A P(A|F,E)}
\end{equation*}

The heart of the translation model is the word translation probabilities $t(f_j | e_i)$.


Algorithm:
\begin{itemize}
	\item EM begins by assuming all word translations $t(f_i | e_j)$ are equally likely
	\item compute probabilities of alignments given word translation probabilities (E-step) following (\ref{eq:1}) and (\ref{eq:2})
	\item compute counts of aligned word pairs, weighted by alignment probabilities (E-step) 
	\item recompute MLE word translation probabilities from these counts (M-step)
	\item repeat
\end{itemize}



\rule{0.3\linewidth}{0.25pt}
\scriptsize

Copyright \copyright\ 2018 Antonio Mallia

\href{https://www.antoniomallia.it}{https://www.antoniomallia.it}


\end{multicols}
\end{document}
