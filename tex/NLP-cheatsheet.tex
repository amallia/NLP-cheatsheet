\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{graphicx}

% To make this come out properly in landscape mode, do one of the following
% 1.
%  pdflatex latexsheet.tex
%
% 2.
%  latex latexsheet.tex
%  dvips -P pdf  -t landscape latexsheet.dvi
%  ps2pdf latexsheet.ps


% If you're reading this, be prepared for confusion.  Making this was
% a learning experience for me, and it shows.  Much of the placement
% was hacked in; if you make it better, let me know...


% 2008-04
% Changed page margin code to use the geometry package. Also added code for
% conditional page margins, depending on paper size. Thanks to Uwe Ziegenhagen
% for the suggestions.

% 2006-08
% Made changes based on suggestions from Gene Cooperman. <gene at ccs.neu.edu>


% To Do:
% \listoffigures \listoftables
% \setcounter{secnumdepth}{0}


% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
	{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}

% Turn off header and footer
\pagestyle{empty}
 

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}


% -----------------------------------------------------------------------

\begin{document}

\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\textbf{NLP Cheat Sheet}} \\
\end{center}

\section{Bag of Words Methods}
\subsection{Information Retrieval}
\subsubsection{Cosine similarity metric}
Define a similarity metric between topic vectors. It is the cosine of the 
angle between the term vectors.

\begin{equation*}
sim(A, B) = \frac{\sum_{i} a_i \times b_i}{\sqrt{\sum_{i} a_i^2} \times \sqrt{\sum_{i} b_i^2}}
\end{equation*}

Unusual words determine the topic much more than common words.
Weight each term frequency $tf_i$ by its inverse document frequency $idf$.
\begin{equation*}
idf_i = \log (\frac{N}{n_i})
\end{equation*}

where $N$=size of collection and $n_i$ = number of documents containing term  $i$
\begin{equation*}
w_i = tf_i \times idf_i
\end{equation*}

\resizebox{\textwidth/3}{!}{ 
$sim(A, B) = \frac{\sum_{w \in A,B} tf_{w,A} \times tf_{w,B} \times idf_w^2}{\sqrt{\sum_{w_1 \in A} (tf_{w_1,A} idf_{w_1})^2} \times \sqrt{\sum_{w_2 \in B} (tf_{w_2,A} idf_{w_2})^2}}$
}

\subsection{Sentiment analysis (opinion mining)}

Opinion mining is the task of judging whether a document expresses a positive or a negative opinion (or no opinion) regarding a particular object or topic. 

The simplest strategy uses a bag-of-words model. We create lists of 'positive' and 'negative' words and judge a document based on whether it has a preponderance of positive or negative words (and judge it neutral or 'objective' if it has few words of either category.

Creating such lists is not easy; some of the words are likely to be quite different for different kinds of topics. As an alternative, we will take a collection of documents on some topic and rate them (by hand) as positive or negative. We will then use this collection to train a classifier model.

$
c = argmax_{c \in C} P(c) \prod_{i \in positions} P(w_i | c)	 
$ \\

$P(c) = \frac{count(\text{docs labeled t})}{N}$ \\

$P(w_i | c) = \frac{count(\text{docs labeled t containing } w_i)}{count(\text{docs labeled t})}$

\textbf{Smoothing}: $P(w_i | c) = \frac{count(w_i, c) + 1}{(\sum_{w \in V} count(w,c)) + |V|}$ \\

The bag-of-words strategy with NB model does quite well for short reviews, but fails for:
\begin{itemize}
	\item ambiguous terms
	\item negation
	\item comparative reviews
	\item revealing aspects of an opinion
\end{itemize}


\section{Context-free grammar}
A context-free grammar consists of:
\begin{itemize}
	\item a set of non-terminal symbols $A, B, C, \dots \in N$
	\item a set of terminal symbols $a, b, c, \dots \in T$
	\item a start symbol $S \in N$
	\item – a set of productions $P$ of the form $N \rightarrow (N U T)*$
\end{itemize}
CFG can be used to generate sentences, recognize sentences or parse sentences.

\section{Parsing}
\subsection{The CKY algorithm}
The	grammar	has	to	be	in	\textbf{Chomsky	normal form (CNF)}. Every production must have the form $A \rightarrow B C$ or $A \rightarrow a$.

\begin{itemize}
	\item Use a parse table $PT[i,j]$
	\begin{itemize}
		\item each entry is a list of grammar symbols
		\item $PT[i,j]$ includes $x$ if the words from $i$ up to (but not including) $j$ can be analyzed as an $x$
		\end{itemize}		

	\item Two steps
		\begin{itemize}
		\item if there is a rule $A \rightarrow a$ and word $i$ is an $a$, put an $A$ in $PT[i, i+1]$
		\item if there is a rule $A \rightarrow B C$, $PT[i,j]$ includes $A$ and $PT[j,k]$ includes $B$, add $C$ to $PT[i,k]$
		\end{itemize}		
\end{itemize}



Time complexity = $O(n^3)$ \\
Space complexity = $O(n^2)$

\section{POS tagging}
\section{HMMs and the Viterbi decoder}
HMM is a Weighted Finite-state Automaton (WFSA)
\begin{itemize}
	\item Each transition arc is associated with a probability
	\item The sum of all arcs outgoing from a single node is 1
\end{itemize}

Viterbi decoder's time complexity is $O(s^2 n)$ and the space complexity is $O(s n)$ for $s$ states and $n$ words.

$viterbi[s, t] = max_{s'}( viterbi[s', t-1] \times p(s, s') \times P (token[t] | s) )$

for each $s, t$: record which $s’, t-1$ contributed the maximum.

\section{Chunkers and name taggers}
\section{Maximum entropy}
\section{Lexical semantics and word sense disambiguation}
\section{Probabilistic CFG}
\section{Learning}
\section{Reference resolution}
Identify all phrases which refer to the same real-word entity.

\subsection{Terminology}

Referent: real world object referred to 

Referring expression [mention]: a phrase referring to that object

Coreference: two expressions referring to the same thing.

Antecedent: prior expression

Anaphor: following expression

Types of referring expressions: definite pronouns (he, she, it), indefinite pronouns (one), definite NPs (the car), indefinite NPs (a car).

Cataphora: pronoun referring to a following mention

Bridging Anaphora: reference to related object

Zero Anaphora: many languages allow subject omission, and some allow omission of other arguments

Non-NP Anaphora: pronouns can also refer to events or propositions

Wikification: entity linking, so map each document-level entity to an entry in a standard database as Wikipedia


\subsection{Hobbs search}
In selecting which sentences to search, Hobbs starts with the sentence containing the anaphor and then selects prior sentences from right to left, examining first the immediately preceding sentence.
However, within an individual sentence he searches from left to right;  this is the way he captures the preference for antecedents in subject position.
The resulting search order defines a Hobbs distance:  if in searching for antecedents of $X$ the $d-th$ candidate we consider is $Y$, we say that the Hobbs distance from $X$ to $Y$ is $d$.

\subsection{Constraints}
\begin{itemize}
	\item animacy
	\item gender
	\item number
\end{itemize}

\subsection{Preferences}
\begin{itemize}
	\item recent: at most 3 sentences back
	\item salient: mentioned several times recently
	\item subjects: 
\end{itemize}


Ge, Hale, and Charniak formula:
$P = P1( correct antecedent is at(Hobbs distance d) \times 
P2( pronoun | head of antecedent) \times
P3( antecedent | mention count) \times
P4( head of antecedent | context of pronoun ) $


\section{Question Answering}
\subsection{Mean Reciprocal Rank}
Typically for evaluations QA systems generate a ranked set of answers to each query and are scored based on the rank of the first correct answer:  mean reciprocal rank

\begin{equation*}
	MRR = (\frac{1}{N}) \sum_i \frac{1}{rank_i}
\end{equation*}
Each question is then scored according to the reciprocal of the rank of the first correct answer. The score of a system is then the average of the score for each question in the set.


\section{Machine translation}
\subsection{The Noisy Channel Model}
Suppose we are translating French/Foreign sentences \textbf{F} to English \textbf{E}
We want to determine the English sentence most likely to have generated it:
\begin{align*}
E' &= argmax_E P(E | F) \\
   &= argmax_E \frac{P(F | E) P(E)}{P(F)} \\
   &= argmax_E P(F | E) P(E)
\end{align*}

It combines \textbf{translation model} $P(F | E)$ and \textbf{language model} $P(E)$.
$P(F|E)$ is learned from parallel corpus, $P(E)$ can be learned from large monolingual corpus.

\subsection{The Lexical Translation Model}
\subsubsection{IBM Model 1}
Translation is based on alignment. Mapping a target word at position $i$ to a source word at position $j$.
Assumptions:
\begin{itemize}
	\item Each target word is generated by exactly one source word 
	\item  A target word can be generated by a NULL word; multiple target words can be generated by the same source word
	\item All alignments are equally likely (a very crude model)
\end{itemize} 

Translation model generates a sentence of F (given E) in 3 steps:
\begin{itemize}
	\item pick a length for F
	\item pick an alignment of F (length J) and E (length I+1) $P(A|E) = \frac{\epsilon}{(I+1)^J}$
	\item pick the $ith$ word of F based on English word $e_j$ with which it aligns using distribution $t(f_i | e_j)$
		$P(F| E,A) = \prod_j t(f_j, e_{a_j})$\\
\end{itemize}

\subsubsection{EM training}
Typically we are given sentence aligned but not word aligned corpora, so we cannot compute these probabilities by direct counting and instead use an iterative procedure called "EM".

\begin{equation} \label{eq:1}
P(F, A | E) = P(F| E,A) \times P(A|E)
			= \frac{\epsilon}{(I+1)^J} \prod_j t(f_j, e_{a_j})
\end{equation}

\begin{equation} \label{eq:2}
P(A| F, E) =\frac{P(F, A | E)}{\sum_A P(F, A|E)}
\end{equation}

\begin{equation*}
P(F|A) = \sum_A P(F, A | E) / P(A|F,E) = \frac{P(F, A | E)}{\sum_A P(A|F,E)}
\end{equation*}

The heart of the translation model is the word translation probabilities $t(f_j | e_i)$.


Algorithm:
\begin{itemize}
	\item EM begins by assuming all word translations $t(f_i | e_j)$ are equally likely
	\item compute probabilities of alignments given word translation probabilities (E-step) following (\ref{eq:1}) and (\ref{eq:2})
	\item compute counts of aligned word pairs, weighted by alignment probabilities (E-step) 
	\item recompute MLE word translation probabilities from these counts (M-step)
	\item repeat
\end{itemize}



\rule{0.3\linewidth}{0.25pt}
\scriptsize

Copyright \copyright\ 2018 Antonio Mallia

\href{https://www.antoniomallia.it}{https://www.antoniomallia.it}


\end{multicols}
\end{document}
